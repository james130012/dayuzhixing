But also Douglas was a key part of
anthropics claude 4 models. It was
really fun to sit down with him on the
day these models got released. We talked
about a bunch of things including how
developers and builders should think
about this next generation of anthropic
models. We talked about what the trend
line mean for where these models will be
in 6 12 months and 2 3 years from now.
We hit on what's required for reliable
agents and when these models will get
better in domains like medicine and law
and kind of mirror the advances they've
already made in coding. And then we hit
on his views on alignment research.
where we are today, what's working, what
still needs to be done, and its reaction
to the AI 2027 work. This was just a
fascinating conversation with a
brilliant mind in LLM research. I think
people will really enjoy it. Without
further ado, here's Shelto.
Well, thanks so much for uh coming on
the podcast. Enjoy it. It's a really
cool little route. Yeah. No, I
appreciate you getting in this uh this
little cave with us. Um it's uh it's
always fun. By the time this podcast
comes out, well, the world will have
Claude 4. I I'm sure people will play
around with it, but I'm curious. You
you're one of the first people to get to
play around with these models. um what
gets you most excited about them? So
they're another step up in software
engineering, that's for sure. And Opus
is like really an incredible software
engineering model. Like more and more I
have these moments where I go and ask
her to do something incredibly
illspecified in like our large monor
repo and it's able to go and do it in
like quite autonomous and independent
way going and discovering the
information and figuring this out,
running a couple tests. Um and that that
blows me away every time. Every time we
get a new set of models, we have to like
reccharacterize a mental model of like
what works, what doesn't. like how has
your model now changed of you know when
you're coding like what you use these
models for and don't you know with this
lead so I think the biggest one is time
horizon expands a little uh so I I think
you can characterize model capability
improvements along two axes one of those
is like the absolute intellectual
complexity of the task um and the other
one is the amount of context or the
amount of uh like successive actions
that they're able to uh to like
meaningfully reason over and and these
models feel substantially better along
the second axis like they're really able
to take multiple actions and and like
figure out what they what information
they need to pull in from their
environments uh and uh and then act on
those. Um so giving it like it's the
time horizon also the support that we
like the like cloud code and this kind
of stuff uh the fact that it now has
access to all of the tools to be able to
do this in a useful way and you aren't
sitting there like copy pasting from a
um you know from a chat box uh is like a
pretty meaningful improvement in that
regard too. There is a wide variety of
tasks where I'm looking at, you know, an
hour plus or like many hours of work
where I would have done and it's just
there churning away in front of me doing
these like in terms of human equivalent
time. People are going to get these
models when this podcast comes out for
the first time. What what is your like
advice on the first thing they should
try? First thing they should try. Um I
mean I think honestly try and plug them
into your work. Uh that's the biggest
one is is like sit down and ask it to do
the same thing the thing that you were
about to do first off in your codebase
that day. um and watch as it you know as
it figures out what information it needs
to pull in and figures out what to do
and I think you'll be pretty pretty
impressed. Yeah. I mean now that you
have these new capabilities obviously
you have tons of people that build on
top of these models like what are you
hoping is like newly enabled for you
know builders that take these models and
uh and you know build applications. Um,
so I think there's this there's this
concept of like a product exponential in
some respects where you have to be
constantly building just ahead of the
model's capabilities. And I like to
think about this in terms of like say
like cursor and windurf and devon and
these products. If you look at cursor,
they had a vision for what coding would
be that was substantially ahead of where
the sort of model capabilities were for
a lot of like for a while. You know,
cursor didn't hit PMF until the
underlying models like claw 3.5 sonnet
uh took off such that the assistance
that they wanted to give people was like
able to like be realized. Um and then uh
Windinsurf went I would say like
substantially more agentic and that
enabled them to get a reasonable slice
of market share by really pressing
harder on that product exponential. Uh
what we're starting to see now with
claude code but also with uh the new cla
GitHub integration and with open AAI's
um you know codeex uh and also Google's
coding agent. So you know everyone's
really coding agents. Um is tools right?
uh is people building for an another
level of uh sort of autonomy and
asynchronicity. Um and so right now the
the models are taking these stumbling
steps towards being able to do tasks
independently of you. Uh the kind of
tasks that would have taken you several
hours before. Um what that looks like
next I think is there's this interesting
transferal of you are in the loop every
second to you are in the loop like every
minute to you're in the loop every hour
that we've seen over the course of last
year. And I wonder if it doesn't look
like you're you're managing like a fleet
of like of models in future. And so I
think that kind of interface would be
very interesting to explore like just
how much parallelism can you give
someone when it's not a single model
they're managing but multiple models
doing multiple things and interacting
with each other. I think that would be
pretty exciting. Yeah. Have you seen it
like what what might that look like?
What might that look like? Oh god. I
mean I I know a lot of people actually
at anthropic who are like have multiple
clawed code uh like instances up in uh
different dev boxes which is pretty cool
but but I think I think no one's really
cracked that form factor yet and I think
that's like an interesting form factor
to explore of what is the almost
management bandwidth of an individual.
Um I think this is also an interesting
question to explore from like the future
of like how does economics even work or
like what are the sort of like return on
productivity of these models because uh
if you think like like we initially will
need humans to verify the outputs of
these models and so the economic impact
of the models will be like at like some
initial point bottlenecked by human
management bandwidth until you get to
the point where you can like delegate
the trust uh in a model to itself manage
teams of models. Um and so that like
continual step up in hierarchy of uh of
like abstraction layers um will be I
think uh one of the more important like
trend lines to understand basically you
have uh you know based on the frequency
with which you need to check these
models youating factor you have infinite
number of models running uh and you're
you know if you have to check them every
15 minutes versus every hour versus
every 5 hours uh you can do a lot more
yeah exactly um I think uh Jensen
mentioned this in with respect to like
how he felt about the future of like AGI
progress and this kind of thing. He
said, "Well, actually, I am surrounded
by 100,000 incredibly intelligent uh
AGIS." And he's like, I and and so like
this gives me huge leverage over the
world and and like that's sort of the
impact. And you know, he's describing
how like he himself is like, you know,
this gating factor in like managing the
company of Nvidia and I think a lot of
work ends up looking looking closer to
that direction. Yeah. Who knows? I mean
maybe this like you know this whole
field of org design ends up being
actually the most important uh what is
what exactly and how do you how do you
like trust and and yeah exactly or
structure becomes becomes complicated.
Yeah I know we were saying before the uh
the episode that you did spend a year at
Mackenzie before and I feel like hey
maybe this is a good use case for the
consulting firms you know who have been
doing years of this uh maybe a good new
product line for them. You know,
actually I was really struck by this you
what you just said about how basically
um you know for the app companies it's
about being you know a stage ahead of
like where the models are going and you
know cursor did this the models change
so quickly that it's like almost if you
think about like uh you know what cursor
did versus like the kind of you know
agenda coding companies like cognition
to like maybe someone thinking through
now like what is the you know dashboard
that you use to manage like your 100
team person team what is like the right
level of ahead in your mind to go
because it feels like you may feel like
oh god I'm really out of my skis today
and then in three months you'll be like
actually I'm able where the model
capabilities right um and you have to
constantly reinvent the product to be
like suitable for the like the sort of
frontier of model capabilities um a few
months ahead maybe um I think is a sense
so you still like maintain a lot of
contact with like you know direct users
and this kind of stuff and you can the
product works to some degree but then
it'll it allows you to take advantage of
the the frontier capabilities I feel
like that's the recipe is you know you
have while you're kind of waiting for
the models to get somewhere someone else
is taking up your c the developer love
and your customer case and you know
they'll you know they can probably
integrate in some of the stuff as it
right exactly and and you saw that also
with custom wind stuff and this kind of
thing right there's a lot of things in
these in these models that uh you guys
made progress on like memory uh
instruction following tool use so I
guess you know again kind of
recontextualizing for folks like where
are we in these in these three areas
what works what doesn't yes okay so a
good way to think about what's happened
with these models over the last year is
because RL finally really worked on top
of language models
There's I think no direct ceiling to the
intellectual complexity that we've been
able to like teach these like the
intellectual complexity of tasks with
which we've been able to teach these
models. So you see them doing incredibly
complex math problems, incredibly
complex like coding problems, but those
things are like in scoped domains where
it's like relatively limited context the
problems there in front of the model.
things like memory and tool use. These
are attempts to expand the set of uh
like the sort of context with which the
model is able to um like the context
within which the model is able to act
and the the affordances it has. So this
is things like things like MCP allow it
to suddenly the world opens up to it and
it's able to interact with the outside
world. Memory allows it to run with much
longer context much greater degrees of
personalization than just you know a raw
model with its own context uh window.
And so I think those efforts represent
attempts to uh to to like crack agency
um by giving the model all these
unhoblings in in one respect. Uh and I
think you know the Pokemon Eval is a
quite I love that eval as an avid Game
Boy player back in the day. Um I I feel
like that's it's a it's a great one. I
hope you're going to release that
alongside this uh this model. Yeah.
Yeah. The new one the new model has been
playing Pok√©mon. So you'll see that. Um,
and I think it's a great eval because it
hasn't been trained for. Um, and so it
demonstrates this like generalizability
of intelligence to uh to like I mean a
task which is not completely out of
distribution but one which is like
meaningfully different from anything
it's done before. Um, and another
example I had to buy a strategy guide to
beat that game. I remember like a lot of
ladders and like places to go around it.
Exactly. Um, another example of this
that I really like is uh there's been a
recent interpretability agent that
Anthropic's been working on. And
basically what this does is it does that
job of finding circuits in language
models. And this is really really cool
because one it h we haven't trained for
it to do this. We've trained for it to
you know do you it's a coding agent
right? Um but it's able to mix that with
its knowledge of theory of mind and this
kind of thing to sit there and itself
talk to the model that it's trying to
understand try and reason through what
kind of it has access to tools which are
like looking at visualizing you know
neurons and and circuits and this kind
of thing. And it is actually able to win
this interesting uh uh like alignment
safety eval um which is called the
auditing game where uh you you twist the
model in some way and it has to figure
out what is wrong with the model. Um and
it is able to do that. It's able to talk
to the model you know generate its own
hypothesis about what might be wrong
with the model uh and look at all these
tools. Um I think it's just such a
brilliant demonstration of uh of the
like generalizable you know competence
of of these models with access to tools
and memory this kind of totally I feel
like builders been waiting for you know
agents and you know the ability to kind
of use this stuff reliably. I think
you've talked before on podcast like the
the barit agents is reliability right
how much progress have we made there
like you know uh for the for the
builders that are listeners here. Hey
guys this is Rashad. I'm the producer of
Unsupervised Learning and I just wanted
to take a quick break from the
conversation to remind you to subscribe
to this YouTube channel if you haven't
already. Uh, subscriptions help us grow
and help us continue to bring on the
best guests like Schultto. So, thank you
so much for listening and now back to
the episode. Please subscribe to our
channel. The Barrett Agents is
reliability, right? How much progress
have we made there? Like, you know, uh,
for the for the builders that are
listeners here, I really like the media
for that. I really do think that
measuring success rate over time horizon
is the right way to think about this uh
you know extension of a like agent
capabilities. Um and I I think we're
making a hell of a lot of progress. Uh
we're not 100% there on reliability. You
know these models don't succeed all the
time. There's still a meaningful gap
between the performance of the model
when you ask it to do something once
versus the time versus when you ask it
to you know try 256 times. there's many
evals where you can like completely
solve them um you know with many event
many uh attempts but on the first time
it's not guaranteed.
But that being said I I think every
trend line I'm seeing says that we are
on track to to get like expert
superhuman reliability at uh most things
that we train on. Yeah. What would
change your mind on that? I think if we
were to to basically like fall off trend
line. Um so if let's say by the middle
of next year you started to see some
kind of uh a block on on the time
horizon with which these models are
capable of of acting. Um and I I think
you should look at that like coding is
always the leading indicator in uh in
AI. Um so I think code like you would
see that drop off in in coding first. Um
but that would be perhaps reflective of
uh inherent limitations in the algorithm
which I which I really strongly believe
there aren't. Um there are other
limitations which where the task
distribution might be harder than you
think where because you know there's
less data available for something and it
turns out to be actually quite a
laborious process. Um and so maybe uh if
you think about like computer using
agents then this would be an example
where it's uh that kind of data doesn't
natively exist. But at the same time
also like we're seeing just such
incredible progress there that it feels
to me like relatively I I don't think
that that's the world we're in at all.
Yeah. When do you think I'll have like
one of these just general purpose agents
that can you know fill out all my my
forms for me and you know navigate the
internet for me? Yeah. U one thing I
joke about is like um like personal
admin escape velocity like how how can I
put off doing a task until
as a procrastinator that would be uh
wonderful. Right. Exactly.
Um, uh, it depends. I I still think
there's a meaningful,
uh, this depends a bit on the like
whether or not a company focuses on on
putting like at le like like at least
something like like giving the model
some practice reps. Like if you took a
human off the street and you were like,
you're a general intelligence, but like
I'm going to ask you to do my accounting
and you're not you're not going to make
any mistakes, right? Um, probably the
person you pull off the street makes
some mistakes, right? Um but you know if
they've like done something similar to
it or they're a great mathematician or
something like this then then probably
they will make many or they're a lawyer
or whatever like if there's something
for them to generalize from and map to
then they'll be able to do it with a
much higher degree of likelihood. Um so
I strong depends on what task by the end
of next year I think we should see like
it should be very obvious that this is
near guaranteed um or even by the end of
this year that's like that should be
like pretty clear but by the end of next
year you'll have these things going
around doing a lot of things for you in
your browsers it yeah sounds great to me
your models are really good at coding
like what makes them so uniquely good at
coding is it like a prioritization
internally or like what what you know I
think like people associate anthropics
like oh they're the coding model company
these days what is behind that? I mean,
Anthropic does care a lot about
prioritizing the things we think are
important. Um, and we believe coding is
extremely important. Uh, because coding
is
the that first step in which you will
see AI research itself being
accelerated. Um, and so so we we care a
lot about coding. We care a lot about
measuring progress on coding. We think
it's the most sort of important leading
indicator of model capabilities. Um yeah
I think it's it's a it's a focus is
coding accelerate are these agents
accelerating research today accelerates
me a lot um uh yes basic basically yes
they accelerate the engineering a lot uh
I think it's it's interesting to ask
even people who are utterly brilliant
engineers like how much this is
accelerating a lot of my uh friends who
are like who I would regard as like the
strongest people I've ever worked with
uh they say it's like 1.5x on even on
domains they know well um and on domains
which they don't know well it's like 5x
uh so if it's new program new
programming languages or something you
haven't done for a while it's an
incredible accelerant now it's one like
very important uh like factor to
consider on like how much will AI
accelerate AI progress is how much you
believe that we are computbbounded or
not computed um and so uh and like
whether you think that if you say deploy
AI agents uh who can do the research for
you will that mean that you get sort of
like gains proportional to to the amount
of like researchers that you're now
deploying. At this stage, I imagine it's
like most of these things can do the
annoying parts of your job so you can
think about the brilliant pieces of
research to like go test. I mean, do you
find uh how do you think about the
timeline for where you know these agents
themselves are proposing like
interesting research directions? I mean,
a lot of the work is engineering work.
Um I would say the majority of the work
is engineering work at this point in
time. Uh when they're proposing novel
ideas, I'm not sure to be honest. like
within the next 2 years uh I think
people are already starting to see uh
interesting scientific proposals and
this kind of stuff. I think also if you
like allow an important thing to
consider like the current sort of space
of algorithms these models is they can
become truly expert at something
provided they've had a feedback loop for
that thing. Um, so you need it needs to
have been allowed to practice a little
bit. Um, in the same way that like
humans need to also need to be
relatively easily verifiable, right? Are
we going to get these models that are
like unbelievable coders and like
haven't made the slightest progress in
some of these like more nebulous skills?
Yeah. Um, one point is that ML research
is actually incredibly verifiable. Like
did the loss go down? Right. Uh, so so
if you can get to the point where you
can make meaningful like your proposals
for ML research, you have the best RL
task in the world.
Um even more so I'd say the like general
software engineering in some respects.
Um will we get progress on less
verifiable domains? I'm very confident
that we will. I think uh one interesting
data point here is open's uh recent
paper on um it was like questions to med
like medical questions. Yeah. Um and did
you notice how it was uh eval or how it
was scored? Well, the new med medical
eval right and they had greater
feedback. So they had like all these
questions where it's like the kind of
like a long form answer that you'd have
in a in an exam and they they gave like
points for it. Um and so this is taking
a domain which is not inherently
verifiable in the same way with code or
math is and converting into something
which is much more verifiable. Uh I
think we have I think this is like
reasonably likely to get solved like I
think this is reason likely to already
be solved basically um and near
guaranteed to get solved eventually.
When when's eventually when will we have
like a really good like medical or like
law or I guess like is that just become
part of like the broader model in the
next year? Yeah. Does it become part of
broader model or do you think there's
like you know oh this is like the legal
specific one or medical specific one? Um
I'm I'm a bit of a like large model Maxi
in this respect. I think most most
researchers are most researchers are
yeah exactly I do think there's a lot of
really interesting ways in which
personalization of models matters a lot
right like you want something that is
understands your company understands the
things you care about and understands
you yourself um and so there's a lot of
ways in which tuning models for your
things does matter but I think this
won't be industry specific things so
much as company or individual specific
things uh and you know I think anthropic
has a has a partnership with data bricks
where we're doing company specific stuff
but on at a sort of base level
capabilities. Uh I am firmly believe
that it's it's like single raw uh like
large models and I I think this for a
number of reasons. Um one it's the trend
that we've seen so far. Um but two
because there's no reason in the long
run for the distinction between small
and large models to exist. Like you
should be able to adaptively use the
right amount of like work so to speak,
the right amount of like flops for you
know the difficulty of a given task. Um
and and so I think that that means that
biases towards logical models. It seems
like you're uh you're you're pretty
convinced in in the uh continued
improvement of these models. You I think
a lot of people speculate it's like okay
the models will keep getting better and
then like how does that diffuse into
society and I guess one you know thing
people like to talk about is basically
impact on GDP right and like you know in
the next few years like what impact on
world GDP do you think these models
have? Yeah. Okay. Um I think probably
the the initial impact looks something
like China as a emergence. Um because
there's there's going to be like which
is the thing that has probably most
impacted world in the last 100 years. Um
like you know you look at like Shanghai
over like the course of 20 years and
like dramatically transforms like you
know um but and this will be like
dramatically faster than that but you'll
see that. But there's important
distinctions to be made here. one is
that I think we're near guaranteed at
this point to have effectively models
that are capable of automating any white
collar job um by like 2728 and or near
guaranteed end of decade. That being
said, that's because we have uh like
those are those tasks are
quite susceptible to our current suite
of algorithms like you know you can try
things on computers many times. you can
uh there's like wealth of data available
for this. Um they don't you know the
internet exists but that same resource
of data doesn't exist for say robotics
or for biology. And so for a model to be
a superhuman coder you just need the
affordances which we've already been
able to give the models and you need to
sort of like take the existing
algorithms and like scale it up. For a
model to be a superhuman biological
researcher you need automated like
laboratories where it's able to propose
and run experiments in a hugely
paralyzable way. or for it to become as
competent in the real world as we are,
you need it to be able to act in the
environment through robotics. And so you
need a hell of a lot of robots to
actually like collect the data and and
do that in in a way that unlimited. So
one um mismatch that I think we might
see and I'm I'm actually also worried
about us seeing is you'll see uh a huge
impact on white collar work um and
whether that looks like just dramatic
augmentation uh you know like TBD but uh
you will see that world change a lot uh
and we'll need to pull forward the like
the dramatic transformation of things
that make our lives a hell of a lot
better. So to pull forward medicine to
pull forward abundance in the real world
we need to like actually figure out the
you know the cloud laboratories and the
robotics and this kind of stuff but by
that time we'll have like you know
millions of AI researchers like
proposing they don't need such large
scale of robotics or biological data. So
AI progress goes really fast. Um but uh
we need we need to make sure that we
like pull in the feedback loops of the
real world to actually deliver on on on
like meaningfully changing world GP and
this kind of stuff. Yeah. So you
basically think for like each white
collar profession like they'll be you
know you'll be able to build some sort
of reward model um similar to you know
how it's done in this in the healthcare
valves and you know I think what's
always surprising is how limited data
you need to actually build those things
you know in the same way that like a
human learns to do this on like
relatively limited data right exactly um
and and even I think what we
conclusively demonstrated is that we can
teach models like so far we haven't yet
hit like an intellectual ceiling on the
tasks which we're able to teach teach
the models now they do seem somewhat
less sample efficient than humans.
That's also okay because we can we can
run thousands of copies of them in
parallel and they can be interacting
with different variations of task like
they can you know have lifetimes of
experience. Um and so it's okay if
they're more like less sample efficient.
Yeah. Because you you still you still
get like expert human reliability and
performance of that task. It seems like
you think this like you know uh this
paradigm kind of gets us you know pretty
much all the way there. Yeah. Um you
know obviously you have folks like Ilia
who have been saying look you know there
needs to be some sort of other
algorithmic breakthrough. What's the
other side here? Yeah, makes sense. I
think most people in the field currently
believe that are like the the like
pre-training plus RL like sort of
paradigms which we've explored so far to
like themselves sufficient to reach AGI.
Uh this like we haven't seen the trend
lines like bending yet like this like it
works like this this sort of combination
of things. Um whether there are other
mountains to kind climb that could get
us there faster. It's entirely possible.
I mean, Ilia's invented like maybe both
of these paradigms before. So, who am I
to bet against him, right? Uh, every
piece of evidence I see says that these
are sufficient. Um, you know, maybe Ilia
is betting that way because he wants to
uh, you know, like he doesn't have the
capital, like as much capital available
or he thinks that this is a better way
to do it. Um, entirely possible like I'm
not going to bet against Ilia, but but I
do think what we have now will get us
there. the limiting factor on this will
be energy compute like when when do you
think we start to bump up against that?
I think there's a great table at the end
of situational awareness which which
details this where like by the end of
the decade we start to report like
really like dramatic percentages of US
energy production um like you're over 20
I think maybe 2028 like 20% of US energy
um and so you can't go orders of
magnitude more than that without dramat
like dramatic changes um this is
somewhere I think we need to to invest
more I think this is one of like the
important vectors along which
governments should act uh Dylan has this
wonderful graph of China's energy
production versus US energy production
and the US energy production is flat and
China's energy production is like this
like they're just doing a much better
job than we are of uh of building out
energy. Um and so yes we yeah I guess in
this current wave of model improvement
like what metrics are I mean you know it
seems like it's it's time horizon based
metrics but like what what are the
things that are worth hill climbing on
right now like you know as you you know
move from four to whatever comes after
four. Yeah. Um I think in general I've
been impressed by internal company
evals. Um there's many companies which
like have devisor their own version of
like sweet bench let's say. Um and uh
these are quite rigorous and like well
held out. So I enjoy hill climbing
those. Um I also think really really
complex tests like frontier math is
really interesting um one to watch over
the next year. Um because that
represents such a ceiling of
intellectual complexity that I think
it's interesting. Um, but more and more
I I think what matters is eval hard to
produce. If we could produce evals which
meaningfully capture the time horizons
of people's like work days, I think that
would be the best thing to produce. But
no one has gone out there and produced
that in public. Um this would another
thing which I think governments should
do because I think understanding what
the trend line looks like is such an
important input into policy and this is
also something which government's a well
place to do where they should be
producing like what does the inputs and
outputs look like of an hour or a day of
a lawyer or an engineer's you know like
you know daily like work day um and can
I can I convert that into something
that's gradable um and so that we can
actually measure progress against it I
guess on the like set of problems that
you have to overcome like as a
foundation model company like where does
like having good eval like rank on the
list. Yeah. Um I mean every foundation
model company has a really big evals
team full of great people working
incredibly hard to do this. Uh I mean I
think like the core there's like the
core algorithmic and infrastructure
challenges of even training the thing
but without good evals uh you don't know
what your progress is at all and and
it's hard to keep external evals like
fully held out. Um, so it's important to
have good internal evals that you trust.
But also, I'm struck by having like
people building applications on top of
your models that are willing to share
like the way they think about eval is
like incredibly helpful to uh because
obviously yeah, especially as you get
into like a lot of these different
verticals that you might want to improve
on. It's like it's hard for you guys to
figure out like what is like the
specific thing in logistics or you know
legal or accounting or whatever it is.
And requires such expertise and taste. I
think that's another one of the stories
of the last couple of years is that you
went from uh model outputs being you
know you could pull anyone off the
street and say hey which which output
you prefer and it would meaningfully
improve the model to needing like grad
students or experts in their field to be
able to improve the outputs of the
models. I mean if you put me put you
know I know some field that I don't know
very well like biology or whatever um
and and put two model outputs in front
of me I would I would struggle on a lot
of them like I don't I wouldn't have the
expertise to know which one is a better
answer. I guess this idea of taste like
I mean I'm struck by like you know uh
the way obviously you've seen memory now
put into a lot of the way that consumers
interact with these models. Um but it
seems like part of the reasons different
AI products seem to have taken off is
like they find some like they struck a
chord and like the zeitgeist of like the
way that they're you know I guess you
guys had this with like your Golden Gate
like example and there's been tons of of
other things like this. What does this
like look like in the future in terms of
like model customization like to the I
guess for the vibe of like the end user?
Yes. Um, well, I think actually there's
a there's a weird sort of future where
these models end up being like one of
your most like intelligent and
charismatic friends in I don't know
about your friends, but already pretty
close to, right?
Um, and so I hope that and I think
almost none of our models are like
they're decent along these axes, but um,
and I know many people who uh, who spend
like a lot of hours talking to Claude
actually, but I I think there's there's
so much further that we could go. Um,
and I think we haven't like we've
explored like 1% of the depth of like
personalization and like understanding
of the model could have of you. How do
you get better at that? that like you
know people that have just like
exceptional tastes like being
opinionated in the way that they're like
steering these models or how would you
even go about solving that? I mean I
think a large part of that reason why
Claud is so good in that way is Amanda
um and like her taste. Um and I think
similar to like beautiful products uh an
important part of that like is singular
taste. Um and you know we've all seen
the sort of perils of uh AB feedback
mechanisms and like thumbs up thumbs
down just like lead you down a dark path
basically. I think in part these models
are are such wonderful like simulators
in some respects of like you know
they've been asked to model the entire
entire distribution of the internet. So
I think one of the ways that this solved
is just by providing an extraordinary
amount of context about yourself. The
models should actually almost be
automatically really good at
understanding what you want and then in
designing the personality and this kind
of thing probably uh individuals with
taste and then like you know your own
sort of conversations and feedback with
the model um some combination thereof.
I'm sure you had a bunch of people
playing around with these models before
they were released. Like any stories
that like you particularly uh
particularly resonated. I think it's
just everything has been a noticeable
step up in my like confidence in asking
like turning to the model first I
suppose to um I have also enjoyed I
think uh how like relentless these
models are in some ways. I mean is that
a good word? I don't know. But like um
this is great. So, um, we have this
great eval, um, and, uh, where in this
eval is meant to fail. It's meant like
it's like something on Photoshop or
whatever, and it's like not meant to be
able to do that thing in Photoshop. Um,
and so the model goes, "Oh, well, I know
I can't do this in Photoshop. So, I'm
going to download uh like this Python
like library and I'm going to do it with
the Python library and then upload it
into the Photoshop thing." And look,
hey, I've done it. Um, and so it's maybe
it's not relent like there's a creative
and like mischievous like what expected.
Exactly. Um like I I thought that story
was pretty cute. Um that is really cool.
So what I mean obviously you've got
these new models out today. What are the
next like six 12 months look like in
your best guess? Um so the next 6 to 12
months very much looks like uh you know
scaling up RL um and sort of exploring
where that where that gets us. Uh and I
think you should expect to see
incredibly rapid advances as a result of
this. It it is in in many respects. I
think like Dario outlined this in uh in
his essay about Deepseek where he said
that like comparatively small amounts of
compute have been applied to the the RL
scaling regime compared to the
pre-training regime. Uh and this means
that there's like still such huge gains
to be made even with existing pools of
compute and the pools of compute are
dramatically multiplying this year as
well.
Um so expect to see continual rises in
model capability like expect basically
by the end of this year the coding
agents one good metric will be the
coding agents that are taking their
first like halting steps today uh should
be very competent. Uh you should you
will probably feel very confident in
delegating substantial amounts of work
for hours on like hours of human. What's
going to be your check-in time? Like
what how Yeah, exactly. What does the
check-in time look like? And like at the
moment with CL code, you know, sometimes
it's 5 minutes, sometimes it's like
you're sitting there watching it in
front of you. Uh by the end of the year,
it's probably several hours um of like
confidently doing this for many things.
Uh whereas now, yeah, sometimes it
model's able to do several hours,
sometimes you're able to do huge amounts
of work, but it's spiky. Yeah, I feel
like that's me the game changer things.
I feel like you know one of the lessons
even from like RPA is like you have to
sit there and watch something do your
work. At some point you're like I'd
rather just do this like myself. Yeah,
sometimes right sometimes you step in
and and eventually we'll like be able to
delegate that. Um I think someone
tweeted a little while ago that like the
future of like software engineering
looks like Starcraft and I think when do
we when do we get like Starcraft level
like your your sort of APM of like
coordinating all your pieces? That's
probably the end of the year. So what
does that mean then from like a model
release cadence? I mean if if you guys
are scaling this so quickly like does
that mean that like you know how like
often do you would think people you know
all the labs end up like shipping new
models in this like period of rapid
adjustment? I I would expect to see the
model cadence substantially faster than
last year. Um in many ways 2024 was uh
was a a sort of deep breath in um as
people uh figured out um you know the
the new paradigms and did a lot of
research and and sort of like better
understood what's going on. Um and I
expect 2025 to feel meaningfully faster.
Uh where um particularly also because as
you as models get more capable the the
set of like reward uh available them to
them expands in important ways. You know
if if the if you have to give feedback
on every single like sentence that it
outputs, right? Um this is this is very
not very scalable. Um, but if you're
able to allow it to do hours of work in
such a way that it you can just judge
did it complete the thing I wanted, did
it do the right piece of analysis, did
the website work, and were people able
to, you know, message on it and this
kind of stuff. Um, it means that
the basically it should be able to climb
these like rungs of the ladder ever
faster even though the um the complexity
of the tasks is increasing. You
mentioned earlier there's like open
codeex, there's you know Google jewels,
there's like all this different stuff.
There's all these startups building out
like and we're actually we're launching
a GitHub agency. you'll be able to like
anywhere on GitHub, you'll be able to
say, "Hey, at Claude um and and we'll,
you know, spin off and do some work for
you." Yeah. So, everyone is like
competing for the uh for the hearts and
minds of developers. Like, what do you
think will determine, you know, which uh
tools and models developers use? Um I
mean, I think a big part of it
relationship between the companies and
developers um and and you know how much
trust you you each other. Big part also
is that like trust and respect right um
between the companies and developers. Um
I think a large part is also the model
capabilities. Um which ones the models
which ones people are actually like
comfortable happening enjoy using like
you know the personalities and the
ability like the competency of the model
and the trust you have in it to go off
and do these tasks for you. Um and I
hope also that uh you know over time um
as the the stark capabilities of these
models become more and more and more
apparent uh that the like mission of the
company as well becomes you know
important and that like you think of
like which companies you're working with
as a as like who you're sort of trying
to build the future with. I'm sure I
mean it's like you know especially if
the cadence of releases keeps going up
it's like every you know every month
people will be inundated with like well
this one climbed on this email and that
one climbed on that eval. This is
actually I think this is like in an
interesting way this is one of the
things people didn't expect about um
like you know GPT rappers right is that
one of the benefits of wrapping lang the
model companies is that you can surf the
frontier of model capabilities. Oh 100%
I feel like everyone that tried to not
be a rapper just lit up money on fire.
Right. Exactly. Um and so like surfing
that frontier of model capabilities is
really wonderful. There's a there is a
like a reverse effect where uh there are
certain things you can only you can you
can sort of only predict uh if you have
access to underlying models like you you
can really feel and see the trend lines
or you can only build um if you if you
like I think all of the deep research
equivalents took some amount of RL in
such a way that like it was hard to
build a deep research equivalent product
um from outside one of the uh the labs.
Can you just explain that actually like
why is that because obviously like you
know increasingly I think what open eyes
is RFT I'm sure you guys have some
equivalent it seems like they're opening
up like you know uh to the outside world
like what I guess this is actually a big
question that I think about a lot of
people think about is like what are like
the labs going to be like uniquely good
at building and then what is kind of
fair game for anybody and the labs will
try but the apps won't be in as good a
position to do um so I think with the
release of RT APIs this changes a bit
right because you you sort of there is
now benefit to uh companies specializing
in uh in domains Um but then there's
also going to be those same uh
centralizing benefits like I think you
know uh at least my understanding is
definitely openi um allows people
to or give some discount I think if they
can also train on the outputs model so
that there is going to be some
centralizing um benefit to being the
company that prod like has the RF API
and that people are fine-tuning on
um and
so what are the labs going to be
uniquely good at I think a very
important part here. So couple couple
dimensions. One is the like main metric
that the labs will be judged on is how
effectively they are able to convert
accelerators and like flops and dollars
like capital into intelligence. Like
that is the most important by far
metric. Um and this is the metric that
has sort of distinguished uh companies
like anthropy, companies like open eye
and deep mind uh from really like the
rest of the pack, right? It's like the
models that are trained by these
companies are better. Uh then the next
most important thing after that I think
will be uh the
like you're going to have these models
going to be like employees pretty
rapidly. It's going to be the trust and
like do you like them and uh do you like
do you trust them to carry out the
things that that you um ask them to do.
Uh so I think that will be an important
differentiator and the personalization
will also be an important differentiator
like how well does the model understand
you and your context and your company. I
was sure about you have people building
like you know general purpose agents on
top of on top of your models right not
being a model company like we'll take
the models off the shelf and we'll we'll
do the orchestration we'll do like
really smart chaining and is that like a
doomed task to some extent you know even
just to articulate like what is the
advantage that the model companies
themselves will have by just like
obviously cost advantage makes total
sense versus the API and like you're
surrounded by people that know looking
know these models deeply well. Yeah. No.
Um I mean I think this is actually a
good thing also, right? Like it
encourages an incredible amount of
competition and like finding the right
form factors and this kind of thing. I
think there are some advantages to the
model companies. I think the you know
having access to the models and like
being able to like you know really make
sure I think the RF APIs don't work
brilliantly at the moment. So there like
still like it's like this whole thing
process. So being able to tune the
models for things you think are
important. Um, but I
think the the like waterline is going to
keep going up basically of like
ultimately you are harnessing this like
intelligence on tap like an an employee
that you're hiring or just like the sort
of raw capability of intelligence. And
so, um, yes, there are going to be, uh,
you know, companies that wrap and and,
you know, uh, that orchestrate these
models. Um, and in many cases, they're
going to do fantastically well. And I'm
not sure actually like who has the
advantage or who doesn't, but uh, the
like underlying trend is going to say
like stay true like there's this raw
intelligence being distilled um, and
made available. And so, um, if a company
successfully wraps uh, you know, like
this API, that's fantastic. It's also
going to face a lot of competition. Like
ultimately all modes disappear in like
in like the sort of like T goes to
infinity in some ways because you'll be
able to like spin up a company on demand
um so to speak. Uh and so I think that's
like it's an interesting and complex
future where uh where does like value
accrete? Is it in the customer
relationship? Is it in like the ability
to orient like you know you know pull
together like is it ability to like
meaningfully convert capital into
intelligence? Who knows? I think our
listeners would be super curious. Can
you describe like what does day-to-day
work like as a as a cutting edge AI
researcher look like these days? Yeah, I
think that's a good question. Um, so the
the fundamental thing that like you are
trying to do these companies is is one
of two things. Uh, it is either
to
develop new compute multipliers. Um and
so that is like the process of doing the
engineering of making the you know the
research workflows really fast and
thinking through what we current like
you know what issues are there with
model or what sort of algorithmic ideas
would we like to be able to express and
you know doing the science of studying
how those develop. Um and so there's
like this this very like integrative
research and engineering um uh like form
of work where it's all about iterating
on experiments and like building
experimental infrastructure and and
making that process as uh as as like
clean and and fast as you possibly can.
Um and then there's the process of
scaling up. Um, and so this comes with
its own host of of research and
engineering challenges where you take
these ideas that you think will work and
so uh and that you've uh you know
debated with all your colleagues about
what the right ones to include in the in
the riskier run um are and and and you
scale this up in a much in a much larger
run uh where this has whole new
infrastructure set of infrastructure
challenges where um you know you're
running you need to be way more failure
tolerant this kind of thing. uh and also
new algorithmic and learning challenges.
Uh so there are things that you are only
going to see at each successive oo of
scale um that you then need to go and
figure out scientific reasons for why
those occur and see if you can sort of
study the early emergence of those and
then um and and then like create
experiments that allow you to address or
or take advantage of those effects uh
and and include those in the next large
run. Yeah. Um so yeah this constant loop
of like pushing on those two axes um in
a way that really combines a lot of
science and engineering. So where do you
use AI throughout that uh one a lot in
the engineering um at the moment like
the the primary way it's helping is like
in engineering uh it is also in
implementing research ideas. Um so I
think one way of like seeing the early
ability of these models to help here um
if you take like a single file
transformer implementation um like you
know Kpathy's min GPD or something like
this and ask the model to implement
ideas that you see in papers you will be
stunned by how good it is. Um it is just
like kind of wild. Um, and then if you
go into like some huge like transformer
codebase and ask it, you'll notice that
it's actually it's a little bit harder.
Like the models struggle a bit more
there, but they struggle less and less
every month. Um, so that's like a good
way to preage the future like distill
the context down to just what matters
and then like ask the model to do this
in the then you'll you'll be like struck
by how good it is at helping you do
research. Basically, you've obviously
been really close to this stuff trying
all sorts of things. Like what's one
thing you've changed your mind on in the
last year? Yeah. uh over the last year I
think the pace of progress in reflected
upwards substantially. um so I think in
last year I think it was you could have
been uncertain about will we need to you
know reach uh you know many more s of uh
pre-training compute before we get the
level of capability that we expect to
see really by the end of this year. Um,
and now the answer to that is
conclusively no. RL works. Uh, and
these, you know, the models will get to
that dropin remote worker by 2027. Um,
you will have incredibly capable models
by then. Um, and so like all of the uh
um both like hopes and concerns like
suddenly become I think like they were
already real and now they're
substantially more real in in many ways.
relatively do you think we end up having
to like massively scale data or like by
the time you've made you know claude 17
and these coding models are so good you
know they find so much algorithmic
improvement that the uh the amount of
data more we need is not not too much
well the models might be like the models
might be good enough then they might
their understanding of the world might
be good enough then that they can like
give enough feedback to encroach the the
robots through things right um there
might be there's this concept of what's
called a generator verifier gap where um
you know if If it's easier for the model
to rate something than it is for the for
the other model to do something, then
you can like improve up to your ability
to like critique or rate. Um, and I
think robotics is like quite potentially
one of the areas where this is true. Uh,
and I think this is like also true of uh
of many domains, but robotics is like
this is starkly true because uh our sort
of progress in understanding the world
has has gone so far ahead of our ability
to manipulate it physically. How would
you characterize the current state of
like uh alignment research? uh
interpretability has gone undergone like
crazy advances. I don't know if you've
been like uh following there's there's
some beautiful uh pieces of work here um
that I've been really really impressed
by um where like last year the state of
models we were just beginning to
discover like superp position and
features and the work of Chris Olar and
his team and just like already uh that
was a significant leap in understanding
but now we actually really meaningfully
have circuits in true frontier models um
and we can characterize their behaviors
uh by uh there's a beautiful paper on
the biology of a large language model
where they do break down the ability of
these models to reason over you know
concepts um and in extremely explicit
terms and we don't have a full
characterization of the models uh and
there's still a lot of difficult cases
here but also the models are uh the
models are like quite
good uh one important dynamic to like
explain here is that based on
pre-training the models are quite good
at just generally ingesting human values
like they're quite off pre-training
they're quite default aligned in many
ways. Um, off of RL that's no longer
guaranteed to be the case because you're
putting these models in, you know, that
same model that is like, hey, I
downloaded the Python code like and like
hacked around the fact that I was meant
to fail this test like uh is the kind of
model that the kind of learning process
that means the model will do anything to
achieve the goal um that's been given.
And so overseeing that is like itself a
tricky process that everyone is
currently learning to go through. Yeah,
I mean obviously there was like I feel
like you know about a month ago like AI
2027 came out. A lot of people were
talking about that. Like what was your
reaction to that? Honestly it felt very
plausible. Um I was reading that and and
for a lot of it I was like yeah you know
what
like this might actually be how this
might be how it happens. Um I think
there's like branching possibilities
there but uh and this is maybe like the
20 percentile case but for me um but
also the fact that it's the 20
percentile case is kind of crazy. Like
is it 20% off for you because you find
yourself more bullish on like alignment
research than them or you just think
your timeline is slower? I think I am
more bullish on alignment research than
them um for the most part. Um and maybe
my timeline is like a year or so slower
but also in the like the scheme of
things what is a year like Yeah. Yeah. I
mean depends if you take advantage of
it, right? If you take advantage of it
and you do the right research and this
kind of thing. Yes. If you were kind of
playing policy maker for the day like
what what should we be doing to like
ensure things are on a better path?
Yeah. Okay. So that's a good question.
Um there are the most important thing is
you need to really viscerally feel the
trend lines that we're all seeing and
talking about. And so if you don't then
break down and like understand like
break down all the capabilities you care
about in your country and like measure
the capability of the models to improve
on these like get trend lines that you
would if they were solved then you would
like nation state evals. Yeah. like
nation say like you know you break down
your economy you got like your sort of
like all the jobs that are done in your
country and like convince yourself like
build tests that if the models could
pass them or make meaningful progress
towards tasking them then that would be
your benchmark of intelligence and plot
the trend lines and then go oh my god
what happens in 2027 or 2028. Um the
next thing is uh you should be investing
meaningfully in uh in the in in the
research that we think will like go
towards helping make these models like
understandable uh and uh and steerable
and honest. And so a lot of that looks
like the like science of alignment
basically. Um this actually something
which I've been like sad in some
respects has been driven so much by the
frontier labs. Um there's actually
something which I think that can other
people work on it like you have access
to like you know claude for no no I mean
I think you can make incredible advances
on uh on like interpret and there there
are ways like there's this program
called the maths program where uh people
have done um you know a lot of really
like meaningful alignment research uh
and interpretability in particular um
from uh outside the frontier labs uh but
it's something which I think a lot more
universities should be thinking about.
It's in many respects it is closer to
the pure science of what's going on in
these models. Like this is the biology
and the physics of what is going on in
in language models. Yeah. Why don't you
think there's more?
I'm not sure. I I I really am not sure.
Um I think people have described it to
me as like it's it's been a bit of a
risk. I think the inter mechanistic
interpity like workshop wasn't included
in one of the recent uh conferences like
ICML or something which is crazy to me
cuz it is the closest thing in my
opinion to uh the raw raw science of
what's going on these models. If you
want to like discover the chirality of
uh of DNA or you want to like sort of
like discover general relativity or
something like that for me the tech tree
for that in ML and AI looks like
exploring um uh mechanistic
intervability. Yeah. Um what about in
the good cases like what are we what are
we underthinking uh you know at a
minimum you're saying we're going to
have all white collar jobs automated in
a few years so yeah well um that the
models will be able to do it but
actually like one of the things that's
surprising sometimes I mean not
surprising to you but like uh the world
is surprisingly like slow sometimes to
integrate these things already the model
capabilities are actually like quite
stunning in many ways and if workflows
were oriented around them there still
like even if model capabilities stalled
right
there would still be just a ridiculous
amount of economic value in like
reorienting the world around like using
the current level of capabilities. Um
but anyway, that's a like sort of a side
point. Um this comes back to what I was
saying before about uh we need
to make sure we invest in all the things
that actually make the world better. Uh
so this is like pulling forward the
material abundance.
you know, reaching like, you know, the
scape velocity of uh of admin and this
kind of stuff and like setting up the
models to be able to like do all those
things for us. It's, you know, pushing
forward the boundaries of physics and
and entertain entertainment and this
kind of stuff. And like my hope is that
like people are able to be dramatically
more creative than they are now. like
one of the failure modes I suppose of
our current society is that uh people
consume a lot of like your media and
this kind of stuff but they hopefully
these tools like in the same way you're
able to like vibe code you'll be able to
like vibe create you know a TV show with
your friends or you like vibe create
video game worlds like there should be
like this
intensely people should feel
dramatically more empowered because all
of a sudden you're being given literally
the leverage of entire an entire company
of uh of incredibly talented
um models or individuals. Um and so I'm
excited to see what people do with that.
I think like that is underrated maybe
like there's a aspect of oh yeah god
it's going to like directly replace like
the the things that are currently done
in the economy for work and I think
that's yeah that's very likely but I
also think that everyone should feel
like they will have access to
dramatically more leverage and like the
the world is not solved yet like the
sort of work that is the work that is uh
occurs right now um everyone's like
lives could be dramatically better um
and so solving that I think will become
the interesting challenge yeah I love
that well we always like to enter
interviews with a quick fire round where
we get your takes on some uh overly
broad questions. Uh many of which I
think we've actually already covered
today, but I will uh I'll dig into a few
others. Um what do you think is like
overhyped and underhyped in the AI world
today? Okay. Um let's start with
underhyped. Uh underhyped maybe uh world
models I think are pretty cool. Um and
something that we haven't really
discussed in this in this one. Um, I
think you're you're going to see uh as
technology for like augmented and
virtual reality gets better, you're
going to be able to see these like
models literally capable of generating
virtual worlds in front of you. And I
think that's going to be a pretty like
wild thing. And then that requires like
some sort of like physics understanding.
They're right in like you know cause and
effect and a bunch of things that we
don't seem to have yet. Uh I think we I
think we've demonstrated physics
understanding to be honest.
Yeah. I think that uh there I don't I
think we've meaningfully demonstrated
like cause effect and physics
understanding um both in uh in in eval
of of like physics problems but also in
uh if you watch any of the video models
like they they get physics um uh and and
even like in weirdly generalizable ways
like I saw this great video of someone
asking one of the video models I
remember drawn to put a Lego shark
underwater and it was like reflecting
the light in the right way off the Lego
bricks and it had the shadows in the
right place and this is something it's
never seen before. It's like fully
generalized physics, right? Um that I
thought was pretty cool. Yeah, that
wasn't in the training data. That wasn't
in the training data. There's no there's
no Lego.
Yeah. No Lego sharks under water. Um and
I'm hopeful also that this like same
kind of technology translates towards
things like virtual cells and that kind
of stuff. Yeah. Um so I think that's
exciting. Yeah. You mentioned earlier
that like even if we stopped uh model
improvement today, there's like just you
know tons and tons of applications that
we could build on top or ways to do it.
What do you think like the most
underexplored application are like God I
wish like more people were doing X with
these models or Yeah. Um I mean I think
they've been felt in software
engineering uh because like software
engineers one the models are better at
software engineering. Um but two uh I
think software engineers like more
implicitly understand how to like solve
the problems that they care about.
Um I suspect there's still a lot of uh
headroom in in basically like every
other field. Um and you should sort of
expect to translate the the same like no
one has yet built uh async background
software agent but for like any other
field right um or even really like
anything which comes close to the uh
like feedback loops of uh claude code
and cursor and wind surf and these kind
of things um for any other field. So I
think probably that if anything I guess
people say coding is the ideal you know
problem for these uh these models. It it
is it is it's the leading indicator um
but you should expect everything to
follow basically. That makes sense. I
mean I guess obviously in your time
working on this you know you probably
come to be much more AGI pled than you
were in the beginning. Uh has that
changed at all like the way you like
live your life or plan your life? I
started pretty agile. I read this Gwen
essay that was really important um in in
convincing me uh in in 2020 actually. Um
the last year of of RL progress really
did cause like substantial infection in
that. Um do I live my life that
dramatically differently? No. I work a
hell of a lot. Um I think this is like
the most important thing to work on and
so I like devote my life to it
basically. But um apart from that, I
don't really live my life that
differently. We have this funny joke me
and um uh my friend Trenton. Um one
delineation between us is that I still
wear sunscreen. He doesn't wear
sunscreen anymore. He's like, "No, just,
you know, we'll figure out the biology
of life."
That's good. Good confidence. Yeah. I'm
like, "You know what? Biology is hard."
You know, the feedback loop of biology
are hard. So, I'm I'm wear sunscreen.
Yeah. Just just in case. We hit a wall.
You know, you don't want to be Well,
just in case biology takes 10 years, I
guess. You tweeted a picture of you, I
think, at the Citadel. Um, what was up
with that? Uh, that was a war game. What
does that What does that mean? uh the
was invited to to hang out with some um
people from threeletter agencies and
military cadets and uh it was basically
like a gaming out let's say like a AGI
you know comes along and AI is getting
much better um and what are the
geopolitical implications of that did
you walk away like you know more
terrified or less terrified after that
experience you know maybe a little bit
more terrified
is there enough of that good stuff going
on right now no honestly like I I
I still think that people underrate just
how quickly the next few years are going
to go and and
also how much you should prepare even if
you think it's only a 20% likelihood. Uh
like even if you look at this I'm like
okay wait like there's near guaranteed
like like every trend line I see every
like like every part of the process
could be improved so much that we're
basically like guaranteed to get that
head. Do you think like 90% of anthropic
thinks the same? Yeah. um and GDM and
OpenAI like everyone is very convinced
that we do get drop in remote worker AGI
2027 right
um now that being
said even if you don't have the level of
confidence that the people working at
the labs do and you're still like you
know what it's a 10 or 20% chance you
should still like plan for that like if
you're a government or a country you
should still be like that should still
be the number one issue at the top of
your list of of like how is the future
going to change and I think that isn't
felt enough well this has been
fascinating conversation. I'd love to
leave the last word to you. Like where
can folks go to learn more about you,
the work you're doing at Anthropic? Uh
anywhere you'd like to point them? The
mic is yours. Where should I point them?
Um I mean I think the the thing which
most people should read that they that
maybe like hasn't been read uh is the
interp work. Um I really think like that
basic science of understanding what is
going on in language models is really
quite revealing. Um, and as you sort of
start to see them like compose and
generalize and like build these like
circuits and reason over concepts, um, I
think that will make it feel pretty
real. They're they're long. They're
they're intense, but it's well worth a
read. I think that's that's fun.
Amazing. Well, thanks so much. This was
awesome. Thank you very much. It was
great.